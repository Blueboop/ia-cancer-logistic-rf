{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e5d342",
   "metadata": {},
   "source": [
    "En esta fase dejamos el dataset listo para entrenar los modelos del proyecto **IA-CANCER-LOGISTIC-RF**. Con base en lo observado en la Fase 1, aplicamos los pasos necesarios para limpiar, transformar y organizar los datos, evitando problemas como *data leakage* y asegurando que cada modelo reciba la información en el formato adecuado.\n",
    "\n",
    "El objetivo principal es construir las matrices de entrenamiento y prueba que usarán la **Regresión Logística** y el **Random Forest**, teniendo en cuenta que la primera requiere datos escalados, mientras que el segundo puede trabajar con las características originales.\n",
    "\n",
    "Aquí realizaremos las tareas esenciales del preprocesamiento: eliminar columnas irrelevantes, codificar la variable objetivo, separar predictores y etiquetas, dividir el dataset, escalar únicamente lo necesario y guardar el escalador para su uso posterior. Con estos pasos completados, los datos quedarán preparados para iniciar la Fase 3, donde entrenaremos los modelos supervisados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c557240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo (ajústala si tu estructura es distinta)\n",
    "ruta = \"../data/Cancer_Data.csv\"   # si el notebook está en notebooks/\n",
    "# ruta = \"data/Cancer_Data.csv\"    # si el notebook está en la raíz del proyecto\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1945d7",
   "metadata": {},
   "source": [
    "A continuación, se eliminarán las columnas que no aportan información relevante al modelo. Este tipo de depuración es parte esencial del proceso de construcción de modelos, ya que permite quedarse únicamente con las variables útiles para la predicción. Mantener columnas como id podría llevar a que el modelo aprenda patrones irrelevantes o simples ruidos del dataset, por lo que retirarlas asegura un entrenamiento más limpio y coherente con los objetivos del análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696db55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar columnas que no aportan valor al modelo\n",
    "df = df.drop(columns=[\"id\", \"Unnamed: 32\"], errors=\"ignore\")\n",
    "\n",
    "# Verificar que ya no existan\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e817a",
   "metadata": {},
   "source": [
    "En las siguientes líneas de código se realizará la codificación de la variable objetivo para convertirla a formato numérico. Este paso es necesario porque modelos supervisados como Regresión Logística y Random Forest requieren que la salida sea numérica y no categórica con letras. Además, este tipo de codificación (0/1) es el estándar en problemas de clasificación binaria y coincide con los ejemplos trabajados en el material del curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b701ea07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis\n",
       "0    357\n",
       "1    212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codificación binaria de la variable objetivo\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({\"B\": 0, \"M\": 1})\n",
    "\n",
    "# Verificar que la transformación quedó correcta\n",
    "df[\"diagnosis\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33738d6e",
   "metadata": {},
   "source": [
    "A continuación, se separarán las variables predictoras y la variable objetivo para preparar el dataset en el formato adecuado antes de realizar la división en entrenamiento y prueba. Esta estructuración es la que se utiliza de manera estándar en los flujos de trabajo de Python y coincide con los ejemplos vistos en clase: por un lado, X contendrá únicamente las características numéricas que se usarán para hacer la predicción, mientras que y almacenará los valores codificados (0/1) correspondientes al diagnóstico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1288973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar X (predictores) e y (variable objetivo)\n",
    "X = df.drop(columns=[\"diagnosis\"])\n",
    "y = df[\"diagnosis\"]\n",
    "\n",
    "# Revisar dimensiones\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59036b9",
   "metadata": {},
   "source": [
    "A continuación, se realiza la división del dataset en entrenamiento y prueba. Es importante hacerlo antes de aplicar cualquier tipo de escalamiento para evitar data leakage, tal como se recomienda en los flujos de trabajo correctos de entrenamiento. De esta manera, las transformaciones se ajustan únicamente con los datos de entrenamiento y luego se aplican al conjunto de prueba. Además, se utiliza el parámetro stratify=y para mantener el mismo balance de clases en ambos subconjuntos, asegurando una evaluación más justa del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fac77ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30), (455,), (114,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# División en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.20, \n",
    "    random_state=42, \n",
    "    stratify=y   # mantiene la proporción de clases\n",
    ")\n",
    "\n",
    "# Verificar tamaños\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bddb19",
   "metadata": {},
   "source": [
    "La salida mostrada corresponde a las dimensiones generadas por train_test_split. El conjunto X_train contiene 455 observaciones y 30 variables predictoras, lo que representa aproximadamente el 80% del total y será el que utilicemos para entrenar los modelos. Por su parte, X_test incluye 114 observaciones con las mismas 30 características y corresponde al 20% restante, reservado exclusivamente para evaluar el desempeño del modelo. De forma paralela, y_train y y_test almacenan las etiquetas asociadas a cada conjunto, con 455 y 114 valores respectivamente, codificados como 0 (benigno) y 1 (maligno). Esta distribución confirma que la partición se realizó de manera correcta, manteniendo el equilibrio original entre clases gracias al uso del parámetro stratify=y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dde84f",
   "metadata": {},
   "source": [
    "# Estadatizar X para la regresión logísitica\n",
    "En las siguientes líneas de código vamos a aplicar el **escalamiento solo a las variables X que usaremos en la Regresión Logística**, dejando intactas las versiones originales para Random Forest. Para ello, el `StandardScaler` primero “aprende” la media y la desviación estándar de cada columna usando únicamente `X_train`, y luego utiliza esos mismos valores para transformar tanto `X_train` como `X_test`. De esta forma evitamos filtrar información del conjunto de prueba durante el entrenamiento y, al final, tendremos dos conjuntos de features: `X_train_log` y `X_test_log` (escalados para la Regresión Logística) y `X_train`, `X_test` sin escalar para Random Forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf0c65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Crear el objeto escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar el escalador SOLO con los datos de entrenamiento\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transformar train y test con ese mismo escalador\n",
    "X_train_log = scaler.transform(X_train)\n",
    "X_test_log  = scaler.transform(X_test)\n",
    "\n",
    "# Verificar dimensiones\n",
    "X_train_log.shape, X_test_log.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a661e65",
   "metadata": {},
   "source": [
    "ahora, vamos a guardar el `StandardScaler` que acabamos de entrenar. Esto es importante porque, cuando el modelo se utilice en fases posteriores o en una aplicación final, necesitaremos aplicar exactamente la misma escala a cualquier dato nuevo. Guardar el objeto `scaler` en un archivo `.pkl` permite reutilizarlo sin tener que entrenarlo nuevamente, siguiendo el flujo real de trabajo en Machine Learning: preprocesar → entrenar → guardar → usar para predecir. Para ello, exportaremos el `scaler` dentro de una carpeta llamada **models/** y así podrá ser cargado fácilmente en la Fase 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Guardar el scaler entrenado\n",
    "joblib.dump(scaler, \"../models/scaler_logreg.pkl\")\n",
    "\n",
    "print(\"Scaler guardado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306475e",
   "metadata": {},
   "source": [
    "# Organizar las variables que se usaran en el modelo\n",
    "A continuación, vamos a dejar organizado y totalmente explícito qué conjuntos de datos usará cada modelo. Este pequeño bloque sirve para mantener una estructura clara antes de pasar a la Fase 3, evitando confusiones y asegurando que el flujo del proyecto se mantenga limpio y profesional. Además, es la transición natural dentro del pipeline: después de preprocesar los datos, dejamos definidos los insumos exactos con los que entrenaremos tanto la Regresión Logística como el Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f921ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjuntos finales listos para entrenamiento.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Conjuntos finales para Fase 3\n",
    "# ----------------------------\n",
    "\n",
    "# Para Regresión Logística (requiere datos escalados)\n",
    "X_train_logreg = X_train_log\n",
    "X_test_logreg  = X_test_log\n",
    "\n",
    "# Para Random Forest (NO requiere escalamiento)\n",
    "X_train_rf = X_train\n",
    "X_test_rf  = X_test\n",
    "\n",
    "# Las etiquetas son las mismas para ambos modelos\n",
    "y_train_final = y_train\n",
    "y_test_final  = y_test\n",
    "\n",
    "print(\"Conjuntos finales listos para entrenamiento.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6ab5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conjuntos de entrenamiento y prueba guardados en data/processed/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crear carpeta para datos procesados (si no existe)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# Guardar conjuntos para Regresión Logística\n",
    "joblib.dump(X_train_logreg, \"data/processed/X_train_logreg.pkl\")\n",
    "joblib.dump(X_test_logreg, \"data/processed/X_test_logreg.pkl\")\n",
    "\n",
    "# Guardar conjuntos para Random Forest\n",
    "joblib.dump(X_train_rf, \"data/processed/X_train_rf.pkl\")\n",
    "joblib.dump(X_test_rf, \"data/processed/X_test_rf.pkl\")\n",
    "\n",
    "# Guardar variable objetivo\n",
    "joblib.dump(y_train_final, \"data/processed/y_train_final.pkl\")\n",
    "joblib.dump(y_test_final, \"data/processed/y_test_final.pkl\")\n",
    "\n",
    "print(\"✅ Conjuntos de entrenamiento y prueba guardados en data/processed/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
